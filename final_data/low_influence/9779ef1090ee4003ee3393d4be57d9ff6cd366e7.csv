citationID,citationName,citedReferences
cfaabc4b4ef5254a2f05acb3971ac26706479865,"Multi-Step Optimal Tracking Control of Unknown Nonzero-Sum Games based on Least Squares and Linear Programming: An Application to a Fully-Automated, Dual-Hormone Artificial Pancreas",e90f7770526409ee7d5464eac740e72df4a2e133
5167807250f0874e2e149b0b7640edc267151d31,Data-Based Optimal Microgrid Management for Energy Trading With Integral Q-Learning Scheme,03b7e51c52084ac1db5118342a00b5fbcfc587aa$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c$$@$$a66b0a309d9b27310442df2ac34e4f34a7f19a48$$@$$ecc297a7439b7a11677c4fc4b24e847d98517f5a
5b7a0799212065d890890ed60e83bebf27687e44,Fully cooperative games with state and input constraints using reinforcement learning based on control barrier functions,66d9de49635c8db4931a903228caa744b972d871$$@$$a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
6bc68244e01724242e34a0833beca76706cbc079,H∞output feedback fault-tolerant control of industrial processes based on zero-sum games and off-policy Q-learning,d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95
d08534dc473042a355160013769900ea5d4198b9,Robust control for affine nonlinear system with unknown time‐varying uncertainty under reinforcement learning framework,efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$c5ae4b018e24701651c66d320c7b9a20864eafc9
47323d20442e6cee0a5f6956b6c1e3e4e69e3385,Reinforcement learning for optimal tracking of large-scale systems with multitime scales,efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0$$@$$f43c0c1a36e9659b39a2ba1f311476fc82fbed77$$@$$5a039c0ca6407c3fe375090e36ef0a18364b912e
00c8718282bd3c3327e71246c6d5875bc15fede8,Discrete-Time Optimal Control for Partially Unknown Nonlinear Systems with Asymmetry Input Constraints,d0390315b5ed16af53416b32f0d603eaaf0a9983
91fd39eae946eff67df9e6448a013cc8cfdf8a90,Data-Driven Tracking Control for Multi-Agent Systems With Unknown Dynamics via Multithreading Iterative Q-Learning,b54b9580635a4d9e189b11648d8e84ea523e0128
cb2c8c06e0280b4c272bbe0b3fc7e7e7c3529285,Supplemented with reinforcement learning to improve the detection of passive remote sensing devices,Approximate dynamic programming for real-time control and neural modeling$$@$$66d9de49635c8db4931a903228caa744b972d871$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c$$@$$a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
52288057b90fc7b4d65459f73d51ce5986613f9e,Safe reinforcement learning for discrete-time fully cooperative games with partial state and control constraints using control barrier functions,66d9de49635c8db4931a903228caa744b972d871$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
66bffa8b9ca59acdb70b6260d5e34332befd7424,Safe reinforcement learning for affine nonlinear systems with state constraints and input saturation using control barrier functions,66d9de49635c8db4931a903228caa744b972d871$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
69c5514362d3506fc7c4a00db983251f481dbfe8,Kernel-based multiagent reinforcement learning for near-optimal formation control of mobile robots,b54b9580635a4d9e189b11648d8e84ea523e0128
3000c513f4a36c165f67d9cafe93beab893732b7,Novel two-dimensional off-policy Q-learning method for output feedback optimal tracking control of batch process with unknown dynamics,2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$0f75c3f88263402432826b02587dbc0be5b1d479
0581858ce6d74f9084fe6752d1394ab4300a7492,Model-free event-triggered optimal control with performance guarantees via goal representation heuristic dynamic programming,66d9de49635c8db4931a903228caa744b972d871$$@$$4df845ef004f67585db016200f63cfe62ead4dc8$$@$$c9437662d90cec184b9634230dbee224e1423d20
fa29e1378bb041f6183c7ac23a8221a82aa2a94c,Value iteration and adaptive optimal output regulation with assured convergence rate,4df845ef004f67585db016200f63cfe62ead4dc8$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8
64220cf756126868fe3e0cd036cbce1bf723a21c,Off-Policy: Model-Free Optimal Synchronization Control for Complex Dynamical Networks,efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$66d9de49635c8db4931a903228caa744b972d871$$@$$0f75c3f88263402432826b02587dbc0be5b1d479
8d9f23092fe61b0cdcc1251d29d01fa4bea364b2,Multi-Agent Reinforcement Learning via Adaptive Kalman Temporal Difference and Successor Representation,282001869bd502c7917db8b32b75593addfbbc68$$@$$03b7e51c52084ac1db5118342a00b5fbcfc587aa
18f8ff54d924b40210bc9f06d72dbb3be16b44a6,Model-free nonlinear robust control design via online critic learning.,4df845ef004f67585db016200f63cfe62ead4dc8$$@$$9408c24790d305f770afe3fe0f17e15f389d8ce4$$@$$efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$04f014a6c23eb56bece071b0a5d35e3545cd9685$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e
e495fe008bee4291f918230212a7aa80e8b5f7a2,Data-based decentralized learning scheme for nonlinear systems with mismatched interconnections,9408c24790d305f770afe3fe0f17e15f389d8ce4$$@$$04f014a6c23eb56bece071b0a5d35e3545cd9685
ce27b60f64ff2dd95a88bf2e1da815340dfd6104,Data-Driven Iterative Adaptive Critic Control Toward an Urban Wastewater Treatment Plant,Approximate dynamic programming for real-time control and neural modeling
cd5be5b56d166bb6ae39c308d8de33da4ec5f5f3,Optimal control for unknown mean-field discrete-time system based on Q-Learning,b617cc65c21190bbf3f385e01639af5363660d1a$$@$$a66b0a309d9b27310442df2ac34e4f34a7f19a48$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e
aca1ee36123b88e844cb760417b78e15d3fd897e,H∞-Based Minimal Energy Adaptive Control With Preset Convergence Rate,efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8
c982088b872686728f93aeaa71f4b0b1b0656ec9,Data-driven tracking control approach for linear systems by on-policy Q-learning approach,c9437662d90cec184b9634230dbee224e1423d20$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0
1eff83285ce60846e7bb81fbd1c1a8117d1aa89d,Higher-order Polynomial Signal Tracking Control of Unknown Systems using Off-policy Integral Reinforcement Learning,efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$b617cc65c21190bbf3f385e01639af5363660d1a
4b87e1bcf7e22eddb149bd84ea97b2458a668e45,Modified Q-Learning Algorithm with Lifting Method for Discrete-Time Linear Periodic Systems,2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63
8181ef18ff97db58a5f66418b42c8a4412b79840,Model-Free Optimal Output Regulation for Linear Discrete-Time Lossy Networked Control Systems,c9437662d90cec184b9634230dbee224e1423d20$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8
794002a14b1525a98b643d24ac800b23bf90a4c7,Cooperative adaptive optimal output regulation of nonlinear discrete-time multi-agent systems,0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8$$@$$a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
86cd7389f5032d9b21c6012cc9b34a7a29b3a06e,Adaptive Optimal Trajectory Tracking Control Applied to a Large-Scale Ball-on-Plate System,c9437662d90cec184b9634230dbee224e1423d20$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$c5ae4b018e24701651c66d320c7b9a20864eafc9
9a50dd1d2c8d22196bd19d4255d9634ad69ce99b,Robust optimal tracking control for multiplayer systems by off‐policy Q‐learning approach,c9437662d90cec184b9634230dbee224e1423d20$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0f75c3f88263402432826b02587dbc0be5b1d479
c11e7b29573b1b5040a6ed7339138ef8e8d39cf1,Robust Optimal Tracking Control for Linear Systems via Adaptive Dynamic Programming method,c9437662d90cec184b9634230dbee224e1423d20$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8
62e71b05582833d7c2f83adb606e4c5645d7a70b,Optimal Output Regulation of Linear Discrete-Time Systems With Unknown Dynamics Using Reinforcement Learning,9408c24790d305f770afe3fe0f17e15f389d8ce4$$@$$Approximate dynamic programming for real-time control and neural modeling$$@$$efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$04f014a6c23eb56bece071b0a5d35e3545cd9685$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8
0a56bb37ab28a39c31fe2572595368398d7a2e75,Approximate Policy-Based Accelerated Deep Reinforcement Learning,b54b9580635a4d9e189b11648d8e84ea523e0128
efef29668ae2194290ebbbe6a4d186943ece813a,MM-KTD: Multiple Model Kalman Temporal Differences for Reinforcement Learning,282001869bd502c7917db8b32b75593addfbbc68$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$03b7e51c52084ac1db5118342a00b5fbcfc587aa
55f621b845b63b7911b4b130311099f973206d0f,New Methods for Optimal Operational Control of Industrial Processes Using Reinforcement Learning on Two Time Scales,a347c8ee70c2a184c8f6cd7816e90a1ac235bf11$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$f43c0c1a36e9659b39a2ba1f311476fc82fbed77$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8$$@$$5a039c0ca6407c3fe375090e36ef0a18364b912e
2bfc17757bf9d87bc0b3ad3dd77e8bfaf87ea850,Safe reinforcement learning for dynamical games,5f9c8962af54e8ed580bb3e817cb5376161ab3e8$$@$$0f75c3f88263402432826b02587dbc0be5b1d479
c1644214f472d32d1761cf16b30dbaa53ff44000,A Q-learning predictive control scheme with guaranteed stability,03b7e51c52084ac1db5118342a00b5fbcfc587aa$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$66d9de49635c8db4931a903228caa744b972d871
4714e18e837dc75f3d8f19bf0d7dba8251fcf0b4,Data-Driven Control of Unknown Systems: A Linear Programming Approach,c9437662d90cec184b9634230dbee224e1423d20$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$a66b0a309d9b27310442df2ac34e4f34a7f19a48$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c
283ba1382a53cc5696911806b4a3cf3399a1c831,Robust Policy Learning Control of Nonlinear Plants With Case Studies for a Power System Application,Approximate dynamic programming for real-time control and neural modeling$$@$$e90f7770526409ee7d5464eac740e72df4a2e133
625df5c94d9e0b31ad7cf780f13e713f658b2454,Networked controller and observer design of discrete-time systems with inaccurate model parameters.,d2f763d63e0849d872aaa776ba0a35598ddeb955$$@$$4df845ef004f67585db016200f63cfe62ead4dc8$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$04f014a6c23eb56bece071b0a5d35e3545cd9685$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8
9d4fcaab46fde1e16b5edaec021e26b76cda73b1,Policy Iteration Q-Learning for Data-Based Two-Player Zero-Sum Game of Linear Discrete-Time Systems,b617cc65c21190bbf3f385e01639af5363660d1a$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e
71105a456c4de069a9de5b4e2866a74a16fb8c27,A hybrid control approach for the cracking outlet temperature system of ethylene cracking furnace,f43c0c1a36e9659b39a2ba1f311476fc82fbed77
6f7c739f7c322fe1d890cf4409ee180594238152,Switching control of morphing aircraft based on Q-learning,4df845ef004f67585db016200f63cfe62ead4dc8$$@$$b617cc65c21190bbf3f385e01639af5363660d1a$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e
832566c45e61d55ad425ee98a114a52b53b05b19,Nonzero-Sum Game Reinforcement Learning for Performance Optimization in Large-Scale Industrial Processes,4df845ef004f67585db016200f63cfe62ead4dc8$$@$$efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$a66b0a309d9b27310442df2ac34e4f34a7f19a48$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$bd8633add022930c555dbbce43972826b11130ed$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c$$@$$a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
b3eb93c85b68972ceb7152730ee60a5320642401,Discrete-Time Multi-Player Games Based on Off-Policy Q-Learning,b617cc65c21190bbf3f385e01639af5363660d1a$$@$$dc3e905bfb27d21675ee1720413e007b014b37d3$$@$$9408c24790d305f770afe3fe0f17e15f389d8ce4$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0$$@$$282001869bd502c7917db8b32b75593addfbbc68$$@$$04f014a6c23eb56bece071b0a5d35e3545cd9685$$@$$01f6b4cb627f52f74e7a4d9a5432bf63d5eab85d$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$1db6c50ac6f4fdfbabc75de092f9b59e10cb0ffa$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$bd8633add022930c555dbbce43972826b11130ed$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8$$@$$ecc297a7439b7a11677c4fc4b24e847d98517f5a
6ddc0ed7e92375ecc5e525980fed10fc84b40214,Approximate neural optimal control with reinforcement learning for a torsional pendulum device,4824a3b2c64b8ef27b8630555b1d1e9ac615deaf$$@$$4df845ef004f67585db016200f63cfe62ead4dc8$$@$$Approximate dynamic programming for real-time control and neural modeling$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$ecc297a7439b7a11677c4fc4b24e847d98517f5a
be41207f621b832954084456271c35644f1712e7,Adaptive learning: robust stabilization of two-player games with unmodeled dynamics,9408c24790d305f770afe3fe0f17e15f389d8ce4$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$c5ae4b018e24701651c66d320c7b9a20864eafc9
d514c5a2d36bf7ad4058d5e9964fd70308fdfd87,UAV Motion Strategies in Uncertain Dynamic Environments: A Path Planning Method Based on Q-Learning Strategy,03b7e51c52084ac1db5118342a00b5fbcfc587aa
51f28a8e71d26b09801fc8a9736cd935d1d305f8,Event-based online learning control design with eligibility trace for discrete-time unknown nonlinear systems,a347c8ee70c2a184c8f6cd7816e90a1ac235bf11
41ed15e7251cb1ccbf289020d2340c0991baecbe,Finite-horizon optimal control of discrete-time linear systems with completely unknown dynamics using Q-learning,b617cc65c21190bbf3f385e01639af5363660d1a$$@$$efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e$$@$$ecc297a7439b7a11677c4fc4b24e847d98517f5a
a8cf23ea2f11e7e499e790de1d4715a6dde237d9,H∞ Control for Discrete-Time Multi-Player Systems via Off-Policy Q-Learning,b617cc65c21190bbf3f385e01639af5363660d1a$$@$$efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$04f014a6c23eb56bece071b0a5d35e3545cd9685$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0f75c3f88263402432826b02587dbc0be5b1d479$$@$$1db6c50ac6f4fdfbabc75de092f9b59e10cb0ffa$$@$$bd8633add022930c555dbbce43972826b11130ed$$@$$0139d6bbf799a2c77f6a3b9dd8047b321ab0491e
2dffc8be8b841b4c56b7e043737ff64516fd082e,Off-Policy Q-Learning for Anti-Interference Control of Multi-Player Systems,efed7c026ed54d75539c013ceb6cc5c08d43a045$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$0f75c3f88263402432826b02587dbc0be5b1d479
ddfdd7e62097e14f7aaadbf4dfbc529a1cb5fc81,Output Feedback H∞ Control for Linear Discrete-Time Multi-Player Systems With Multi-Source Disturbances Using Off-Policy Q-Learning,c9437662d90cec184b9634230dbee224e1423d20$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c
e51c7d1ec035f0cd7032b471befaca2569141c85,Data-Driven Optimal Tracking Control for Linear Systems Based on Output Feedback Approach,2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$bd8633add022930c555dbbce43972826b11130ed$$@$$d0390315b5ed16af53416b32f0d603eaaf0a9983
0a2c241ff33697cee7546151899bff12668eb0dc,Optimal Control for Cracking Outlet Temperature (COT) of SC-1 Ethylene Cracking Furnace by Off-Policy Q-Learning Approach,f43c0c1a36e9659b39a2ba1f311476fc82fbed77$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0
6ff3de149c6843c7fade5dfb0e38150e86ecfed7,REINFORCEMENT Q-LEARNING FOR MODEL-FREE OPTIMAL CONTROL Real-time implementation and challenges,9408c24790d305f770afe3fe0f17e15f389d8ce4$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$d4f7edfe6c2796873f68ec26fda771fe5f4e4da0$$@$$282001869bd502c7917db8b32b75593addfbbc68$$@$$66d9de49635c8db4931a903228caa744b972d871$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$0312bdcd4855c566409782b241e2d20ec41e2c95
a136ef5e862b9b32c8c88a224ce7445e61254ac5,Off-policy Q-learning: Optimal tracking control for networked control systems,d2f763d63e0849d872aaa776ba0a35598ddeb955$$@$$b54b9580635a4d9e189b11648d8e84ea523e0128$$@$$c9437662d90cec184b9634230dbee224e1423d20$$@$$2d3d295a0c70931bd5ce6ed0e62a6c5e6f2acf63$$@$$e90f7770526409ee7d5464eac740e72df4a2e133$$@$$bd8633add022930c555dbbce43972826b11130ed$$@$$3c856045d02cc8b77ecabb4b1bf72aa9800aa10c$$@$$5f9c8962af54e8ed580bb3e817cb5376161ab3e8